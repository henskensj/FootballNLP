{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.0.7)\n",
      "Requirement already satisfied: six in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from langdetect) (1.11.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (0.19.1)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.5.7)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.13.3)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
      "Requirement already satisfied: bz2file in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: boto3 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim) (1.7.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.4 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim) (1.10.4)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: python-dateutil<2.7.0,>=2.1 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from botocore<1.11.0,>=1.10.4->boto3->smart-open>=1.2.1->gensim) (2.6.1)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from botocore<1.11.0,>=1.10.4->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "!pip install gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import langdetect\n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(0)\n",
    "#from keras.models import Model\n",
    "#from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "#from keras.layers.embeddings import Embedding\n",
    "#from keras.preprocessing import sequence\n",
    "#from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)\n",
    "\n",
    "%pylab inline\n",
    "#read tweets\n",
    "folder = \"/Users/hovensa/OneDrive - delaware/Data science/WK/\"\n",
    "#df1 = pd.read_csv(folder + \"rodeduivels_2018-06-03.csv\", delimiter = ';')\n",
    "#df2 = pd.read_csv(folder + \"#PorBel_2018-06-06.csv\", delimiter = ';')\n",
    "#df3 = pd.read_csv(folder + \"#BelPor_2018-06-06.csv\", delimiter = ';')\n",
    "#df4 = pd.read_csv(folder + \"DiablesRouges_2018-06-03.csv\", delimiter = ';')\n",
    "#df5 = pd.read_csv(folder + \"RedDevils_2018-06-03.csv\", delimiter = ';')\n",
    "#df6 = pd.read_csv(folder + \"DS_Twitter_KAAGent.csv\", delimiter = ';')\n",
    "#df7 = pd.read_csv(folder + \"DS_Twitter_RSCA.csv\", delimiter = ';')\n",
    "\n",
    "df1 = pd.read_csv(folder + \"#BELBRA_2018-07-10.csv\", delimiter = ';')\n",
    "df2 = pd.read_csv(folder + \"#DiablesRouges_2018-07-10.csv\", delimiter = ';')\n",
    "df3 = pd.read_csv(folder + \"#RodeDuivels_2018-07-10.csv\", delimiter = ';')\n",
    "df4 = pd.read_csv(folder + \"#REDTOGETHER_2018-07-10.csv\", delimiter = ';')\n",
    "#df5 = pd.read_csv(folder + \"#BRABEL_2018-07-10.csv\", delimiter = ';')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dfTemp = [df1,df2,df3,df4]\n",
    "df = pd.concat(dfTemp, ignore_index=\"True\")  \n",
    "df_url = df.text[:].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "df_RT = df_url.str.replace('RT ','',case=True)\n",
    "df_tags = df_RT.str.replace('(@\\w{0,})', '',case=False)\n",
    "df_hash = df_tags.str.replace('(#\\w{0,})', '',case=False)\n",
    "df_col2 = df_hash.str.replace('FACEBOOK LIVE Q&amp;A: ','',case=False)\n",
    "df_trim = df_col2.str.lstrip()\n",
    "\n",
    "df_final = pd.DataFrame({\"created_at\":df.created_at, \"text\": df_trim})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_text = pd.DataFrame(df_final)\n",
    "df_text['Language'] = ''\n",
    "\n",
    "for index,i in df_text.iterrows():\n",
    "    try: \n",
    "        if langdetect.detect(i.text) == 'af':\n",
    "            i.Language = 'nl'\n",
    "        if langdetect.detect(i.text) != 'nl' and langdetect.detect(i.text) != 'fr':\n",
    "            i.Language = 'NA'\n",
    "        else: \n",
    "            i.Language = langdetect.detect(i.text)\n",
    "            \n",
    "    except:\n",
    "        i.Language = 'NA'\n",
    "\n",
    "df_text\n",
    "df_filtered = df_text[df_text['Language'] != 'NA'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hovensa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def DefineEmoticons(language,text):\n",
    "    \n",
    "    emoticondf = pd.read_csv(open(folder + \"emoticons_\"+language+\".txt\",'rt'), delimiter = \"\\t\")\n",
    "    text[\"emotion_\"+language] = \"\"\n",
    "    for index,line in text.iterrows():\n",
    "        for index,row in emoticondf.iterrows():\n",
    "            if(row.Emoticon in line.text): \n",
    "                line[\"emotion_\"+language] = row.Description\n",
    "           \n",
    "    return text\n",
    "\n",
    "text_ad = DefineEmoticons(\"nl\",df_filtered)\n",
    "text_ad = DefineEmoticons(\"en\",text_ad)       \n",
    "text_ad = DefineEmoticons(\"fr\",text_ad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hovensa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\hovensa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#the addition of an emotion word to the tweet text\n",
    "text_ad['emotion']= ''\n",
    "for index,i in text_ad.iterrows():\n",
    "\n",
    "        if i.Language == 'nl':\n",
    "            i.emotion = i.emotion_nl\n",
    "        if i.Language == 'fr':\n",
    "            i.emotion = i.emotion_fr\n",
    "        if i.Language == 'en':\n",
    "            i.emotion = i.emotion_en\n",
    "        else:\n",
    "            '' \n",
    "            \n",
    "text_ad['text'] = text_ad['text'].map(str) + ' '+ text_ad['emotion'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index,tweet in text_ad.iterrows():\n",
    "    if tweet.Language == 'nl':\n",
    "        tweet.text = tweet.text.replace('!',' uitroepteken')\n",
    "        tweet.text = tweet.text.replace('?',' vraagteken')\n",
    "        text_ad.loc[index,:].text = tweet.text\n",
    "    else: \n",
    "        if tweet.Language == 'fr':\n",
    "            tweet.text = tweet.text.replace('!', ' point d\\'exclamation')\n",
    "            tweet.text = tweet.text.replace('?',' point d\\'interrogation')\n",
    "            text_ad.loc[index,:].text = tweet.text\n",
    "\n",
    "        else: \n",
    "            tweet.text = tweet.text.replace('!', ' exclamation mark')\n",
    "            tweet.text = tweet.text.replace('?',' question mark')\n",
    "            text_ad.loc[index,:].text = tweet.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def word2Vec(language,df):\n",
    "    dataSet = df[df.Language == language]\n",
    "    model = gensim.models.Word2Vec.load(folder + language + '.bin')  # you can continue training with the loaded model!    \n",
    "    #create the appropriate format to plot the tweets in a vectorspace\n",
    "    #punctuation can be deleted now since the emotions are already converted\n",
    "    sentences = []\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    for word in dataSet.text: \n",
    "        no_punct = \"\"\n",
    "        for char in word:\n",
    "            if char not in punctuations:\n",
    "                no_punct = no_punct + char\n",
    "        sentence = no_punct.lower().split()\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    #store each vector representation of a word of a tweet in a list 'vectors'\n",
    "    vectors = []\n",
    "    vocab = model.wv.vocab\n",
    "    for w in sentences:\n",
    "        vector = []\n",
    "        for i in w:\n",
    "            if i in vocab:\n",
    "                vector.append(model[i])\n",
    "\n",
    "        vectors.append(vector)\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a function to sum lists\n",
    "def sum_list(l):\n",
    "    sum = 0\n",
    "    for x in l:\n",
    "        sum += x\n",
    "    return sum    \n",
    "def TakeVectorAverage(VectorList):\n",
    "    #to take the average, we need to know how many words each tweet has\n",
    "    count = []\n",
    "    for tweet in VectorList:\n",
    "        count.append(len(tweet))\n",
    "    #sum up each list tp a total that can be divided by the count of the previous step\n",
    "    vectorsAvg = []\n",
    "    for i in range(len(count)):\n",
    "        vectorSum = []\n",
    "        vector = VectorList[i]\n",
    "        if len(vector)>1:        \n",
    "            for j in range(len(vector)):\n",
    "                vectorSum.append(vector[j])\n",
    "            total = sum_list(vectorSum)\n",
    "            average = total/count[i]\n",
    "            vectorsAvg.append(average)\n",
    "        else:\n",
    "            vectorsAvg.append(0)\n",
    "    return vectorsAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the indices and create padded lists for each tweet\n",
    "def tweet_embedding_and_indexing(df,word_vec,MaxLenTweet):\n",
    "    \n",
    "    #flatten the word_vector list in order to be usable in the following steps\n",
    "    flat_list = [item for sublist in word_vec for item in sublist]\n",
    "    \n",
    "    #initiate the tokenizer --> is used to assign indeces to the words in a tweet\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(df.text)\n",
    "    \n",
    "    #how many words can be found in the dataset --> not taking into account those that are not found in the word2vec mechanism \n",
    "    vocab_size = len(flat_list)\n",
    "    #split the tweets into words\n",
    "    encoded_docs = t.texts_to_sequences(df.text)\n",
    "\n",
    "    #fill the tweet lists until max_length is reahced with 0 --> necessary for the creation of the embedding layer\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen= MaxLenTweet, padding='post')\n",
    "    \n",
    "    #create the embedding layer with the weights based on the vector space\n",
    "    e = Embedding((vocab_size), 300, weights= [np.array(flat_list)], input_length= MaxLenTweet, trainable=False)\n",
    "    \n",
    "    return padded_docs,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Dataset(language,df):\n",
    "    tweetVectors = []\n",
    "    yVar = []\n",
    "    MaxLenTweet = 40#max(df.text.apply(lambda x : len(x.split())))\n",
    "    #filter each language subgroup\n",
    "    vectors = word2Vec(language,df)\n",
    "    #Emotion = text_ad[text_ad.Language == language].Emotion\n",
    "    TestIndices,_  = tweet_embedding_and_indexing(df,vectors,MaxLenTweet)\n",
    "    ###vectors = word2Vec('en',text_ad)\n",
    "    ##tweetVectors.append(vectors)\n",
    "    TweetVectorAvg = TakeVectorAverage(vectors)\n",
    "    #final = pd.DataFrame({\"WordVector\": TweetVectorAvg,\"Emotion\": Emotion.tolist()})\n",
    "    final = pd.DataFrame({\"WordVector\": TweetVectorAvg})\n",
    "    \n",
    "    #we want to have all vectors in separate columns\n",
    "    dataf = pd.DataFrame()\n",
    "    dataf = pd.DataFrame(final.WordVector[0]).T\n",
    "    for i in range(1,len(final.WordVector)):\n",
    "        dataf.loc[len(dataf)] = final.WordVector.loc[i]\n",
    "    #dataf['Emotion'] = final.Emotion\n",
    "    #dataf .to_csv('WKDataset ' + language + '.csv', sep=';', encoding='utf-8')\n",
    "    \n",
    "    return dataf,TestIndices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ObtainPred(model,DataSet, sentimentClasses):\n",
    "    prediction = []\n",
    "    if(len(DataSet)>0):\n",
    "        pred = model.predict(DataSet)\n",
    "        prediction = []\n",
    "        if (pred[0][1] <1): \n",
    "            classesT = pred\n",
    "            for j in range(len(classesT)):\n",
    "                    classesT[j][classesT[j] == classesT[j].max()] = 1\n",
    "                    classesT[j][classesT[j] != classesT[j].max()] = 0 \n",
    "\n",
    "            yDF = pd.DataFrame(classesT)\n",
    "            y = yDF.apply(lambda x: int(np.argmax(x)), axis=1)\n",
    "            NrClasses = len(sentimentClasses)\n",
    "            ConvertClass = dict()\n",
    "            for i in range(NrClasses):\n",
    "                ConvertClass[sentimentClasses[i]] = i\n",
    "\n",
    "            bDict = {val:key for (key, val) in ConvertClass.items()}\n",
    "            yDF = pd.DataFrame(classesT)\n",
    "            y = yDF.apply(lambda x: int(np.argmax(x)), axis=1)\n",
    "           \n",
    "            y = y.replace(bDict,regex = True)\n",
    "            y = y.replace(bDict,regex = True)\n",
    "            prediction = y\n",
    "            print(prediction)\n",
    "        else: \n",
    "            prediction = pred\n",
    "    else:\n",
    "        prediction = []\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hovensa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "DatasetNL,IndicesNl = Dataset('nl',text_ad)\n",
    "DatasetFR, IndicesFr = Dataset('fr',text_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenameNL = folder + \"modelnl.json\"\n",
    "filenameFR = folder + \"modelfr.json\"\n",
    "#modelNL = pickle.load(open(filenameNL, 'rb'))\n",
    "#modelFR = pickle.load(open(filenameFR, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = open(filenameNL, 'r')\n",
    "loaded_modelLSTMNl_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_modelNl = model_from_json(loaded_modelLSTMNl_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_file = open(filenameFR, 'r')\n",
    "loaded_modelFr_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_modelFr = model_from_json(loaded_modelFr_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0              hope\n",
      "1              hope\n",
      "2        excitement\n",
      "3          cynicism\n",
      "4        excitement\n",
      "5        excitement\n",
      "6              hope\n",
      "7          cynicism\n",
      "8          cynicism\n",
      "9        excitement\n",
      "10         cynicism\n",
      "11         cynicism\n",
      "12             hope\n",
      "13       excitement\n",
      "14         cynicism\n",
      "15             hope\n",
      "16            anger\n",
      "17            anger\n",
      "18            anger\n",
      "19         cynicism\n",
      "20       excitement\n",
      "21         cynicism\n",
      "22       excitement\n",
      "23       excitement\n",
      "24             hope\n",
      "25         cynicism\n",
      "26             hope\n",
      "27       excitement\n",
      "28             hope\n",
      "29       excitement\n",
      "            ...    \n",
      "11217         anger\n",
      "11218    excitement\n",
      "11219    excitement\n",
      "11220      cynicism\n",
      "11221      cynicism\n",
      "11222      cynicism\n",
      "11223      cynicism\n",
      "11224      cynicism\n",
      "11225          hope\n",
      "11226    excitement\n",
      "11227      cynicism\n",
      "11228         anger\n",
      "11229      cynicism\n",
      "11230      cynicism\n",
      "11231      cynicism\n",
      "11232      cynicism\n",
      "11233      cynicism\n",
      "11234    excitement\n",
      "11235    excitement\n",
      "11236      cynicism\n",
      "11237         anger\n",
      "11238      cynicism\n",
      "11239      cynicism\n",
      "11240         anger\n",
      "11241    excitement\n",
      "11242         anger\n",
      "11243      cynicism\n",
      "11244         anger\n",
      "11245          hope\n",
      "11246      cynicism\n",
      "Length: 11247, dtype: object\n",
      "0                  hope\n",
      "1               neutral\n",
      "2                 anger\n",
      "3                  hope\n",
      "4                 anger\n",
      "5                  hope\n",
      "6                  hope\n",
      "7               neutral\n",
      "8               neutral\n",
      "9               neutral\n",
      "10              neutral\n",
      "11                anger\n",
      "12       dissapointment\n",
      "13              neutral\n",
      "14              neutral\n",
      "15              neutral\n",
      "16              neutral\n",
      "17              neutral\n",
      "18              neutral\n",
      "19           excitement\n",
      "20              neutral\n",
      "21                 hope\n",
      "22              neutral\n",
      "23              neutral\n",
      "24              neutral\n",
      "25                 hope\n",
      "26              neutral\n",
      "27                 hope\n",
      "28                 hope\n",
      "29              neutral\n",
      "              ...      \n",
      "25428          cynicism\n",
      "25429             anger\n",
      "25430           neutral\n",
      "25431           neutral\n",
      "25432             anger\n",
      "25433           neutral\n",
      "25434              hope\n",
      "25435              hope\n",
      "25436             anger\n",
      "25437             anger\n",
      "25438           neutral\n",
      "25439           neutral\n",
      "25440              hope\n",
      "25441              hope\n",
      "25442              hope\n",
      "25443           neutral\n",
      "25444              hope\n",
      "25445              hope\n",
      "25446              hope\n",
      "25447    dissapointment\n",
      "25448           neutral\n",
      "25449              hope\n",
      "25450              hope\n",
      "25451           neutral\n",
      "25452             anger\n",
      "25453          cynicism\n",
      "25454              hope\n",
      "25455             anger\n",
      "25456           neutral\n",
      "25457          cynicism\n",
      "Length: 25458, dtype: object\n"
     ]
    }
   ],
   "source": [
    "PredictionNL = ObtainPred(loaded_modelNl, DatasetNL,[\"anger\",\"cynicism\",\"dissapointment\",\"excitement\",\"hope\",\"neutral\"])\n",
    "PredictionFR = ObtainPred(loaded_modelFr, DatasetFR, [\"anger\",\"cynicism\",\"dissapointment\",\"excitement\",\"hope\",\"neutral\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DfFr = text_ad[text_ad.Language=='fr'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DfNl = text_ad[text_ad.Language == 'nl'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FinalDataNL = pd.DataFrame({\"created_at\":DfNl.created_at, \"text\": DfNl.text, \"prediction\":PredictionNL,\"language\":DfNl.Language})\n",
    "FinalDataFR = pd.DataFrame({\"created_at\":DfFr.created_at, \"text\": DfFr.text, \"prediction\":PredictionFR,\"language\":DfFr.Language})\n",
    "#FinalDataNL.to_csv('FinalDataSet ' + 'nl' + '.csv', sep=';', encoding='utf-8')\n",
    "#FinalDataFR.to_csv('FinalDataSet ' + 'fr' + '.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypyodbc in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from pypyodbc) (39.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in c:\\users\\hovensa\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.1.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pypyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import urllib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "server = 'ikulexdfcr.database.windows.net'\n",
    "database = 'DEMO_WK2018'\n",
    "username = 'demo'\n",
    "password = '{Delaware123;}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "params = urllib.parse.quote('Driver={ODBC Driver 13 for SQL Server};SERVER='+server+';PORT=1443;Trusted_Connection=no;DATABASE='+database+';UID='+username+';PWD='+password+';Encrypt=yes;TrustServerCertificate=no;Persist Security Info = True') \n",
    "engine = create_engine(\"mssql+pyodbc:///?odbc_connect=%s\" % params) \n",
    "conn = engine.connect() \n",
    "tablename = 'TweetsBELJAP'\n",
    "\n",
    "# Push the data\n",
    "FinalDataNL.to_sql(tablename, con=engine, if_exists='replace') \n",
    "FinalDataFR.to_sql(tablename, con=engine, if_exists='append') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probeer tot hier eens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PredictionFR = loaded_modelFr.predict(DatasetFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PredictionFR[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classesT = PredictionNL\n",
    "for j in range(len(classesT)):\n",
    "        classesT[j][classesT[j] == classesT[j].max()] = 1\n",
    "        classesT[j][classesT[j] != classesT[j].max()] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classesT[1:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NrClasses = len()\n",
    "ConvertClass = dict()\n",
    "for i in range(NrClasses):\n",
    "    ConvertClass[classes[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#switch key and value, in order to map words wuth columnindex                \n",
    "bDict = {val:key for (key, val) in ConvertClass.items()}\n",
    "yDF = pd.DataFrame(classesT)\n",
    "y = yDF.apply(lambda x: int(np.argmax(x)), axis=1)\n",
    "print(y)\n",
    "y = y.replace(bDict,regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FinalDataNL[\"created_at\"] =text_ad[text_ad.Language=='nl'].created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FinalDataNL[\"text\"] = text_ad[text_ad.Language=='fr'].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FinalDataNL = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0         cynicism\n",
       " 1         cynicism\n",
       " 2         cynicism\n",
       " 3            anger\n",
       " 4         cynicism\n",
       " 5       excitement\n",
       " 6            anger\n",
       " 7             hope\n",
       " 8            anger\n",
       " 9         cynicism\n",
       " 10        cynicism\n",
       " 11         neutral\n",
       " 12        cynicism\n",
       " 13        cynicism\n",
       " 14        cynicism\n",
       " 15        cynicism\n",
       " 16        cynicism\n",
       " 17        cynicism\n",
       " 18      excitement\n",
       " 19      excitement\n",
       " 20         neutral\n",
       " 21      excitement\n",
       " 22         neutral\n",
       " 23        cynicism\n",
       " 24        cynicism\n",
       " 25      excitement\n",
       " 26        cynicism\n",
       " 27           anger\n",
       " 28        cynicism\n",
       " 29        cynicism\n",
       "            ...    \n",
       " 1198          hope\n",
       " 1199    excitement\n",
       " 1200      cynicism\n",
       " 1201      cynicism\n",
       " 1202      cynicism\n",
       " 1203      cynicism\n",
       " 1204          hope\n",
       " 1205      cynicism\n",
       " 1206          hope\n",
       " 1207         anger\n",
       " 1208      cynicism\n",
       " 1209      cynicism\n",
       " 1210      cynicism\n",
       " 1211      cynicism\n",
       " 1212      cynicism\n",
       " 1213      cynicism\n",
       " 1214    excitement\n",
       " 1215    excitement\n",
       " 1216      cynicism\n",
       " 1217         anger\n",
       " 1218      cynicism\n",
       " 1219      cynicism\n",
       " 1220         anger\n",
       " 1221    excitement\n",
       " 1222         anger\n",
       " 1223    excitement\n",
       " 1224      cynicism\n",
       " 1225         anger\n",
       " 1226          hope\n",
       " 1227      cynicism\n",
       " Length: 1228, dtype: object,\n",
       "                 created_at                                               text  \\\n",
       " 19     2018-07-03 12:46:41                : Le dessin de  de ce mardi           \n",
       " 20     2018-07-03 12:46:38  Je ne me suis tjs pas remise du but de , le bu...   \n",
       " 31     2018-07-03 12:45:42  Si on gagne contre le Brésil, on sera en DEMEY...   \n",
       " 59     2018-07-03 12:44:14  Diffusion des matchs de la coupe du monde de f...   \n",
       " 60     2018-07-03 12:44:10                             Wat een beelden.         \n",
       " 75     2018-07-03 12:43:20  : Bravo à nos  qui font la démonstration que l...   \n",
       " 84     2018-07-03 12:42:10  \"Depuis 1966, au moins 2 demi-finalistes de ch...   \n",
       " 93     2018-07-03 12:41:09  : Matchday point d'exclamation Tous au poste p...   \n",
       " 97     2018-07-03 12:41:02  : ZA-LIG uitroepteken uitroepteken De  zijn go...   \n",
       " 101    2018-07-03 12:40:35  : Quel match  point d'exclamation Quelle persé...   \n",
       " 151    2018-07-03 12:35:42  : C'est exactement ça.\\nAujourd'hui, cette equ...   \n",
       " 198    2018-07-03 12:31:36  : De COSTA a COSTA \\n\\nCourtois  De Bruyne  Me...   \n",
       " 211    2018-07-03 12:30:54  : ZOTTE MATCH uitroepteken  Maak jullie borst ...   \n",
       " 215    2018-07-03 12:30:46                             : Bienvenue A BXL \\n     \n",
       " 228    2018-07-03 12:30:25  : Incroyable  point d'exclamation Les Diables ...   \n",
       " 311    2018-07-03 12:27:02  : Incroyable  point d'exclamation Les Diables ...   \n",
       " 348    2018-07-03 12:25:53  : Incroyable  point d'exclamation Les Diables ...   \n",
       " 349    2018-07-03 12:25:49  : De COSTA a COSTA \\n\\nCourtois  De Bruyne  Me...   \n",
       " 353    2018-07-03 12:25:29  : Incroyable  point d'exclamation Les Diables ...   \n",
       " 357    2018-07-03 12:25:02  : Je n'ai pas les mots  nous sommes en quart. ...   \n",
       " 377    2018-07-03 12:23:13  : Romelu s'érige tant et plus en leader moral ...   \n",
       " 379    2018-07-03 12:22:56  : _ Papa, pourquoi ma soeur s'appelle Rose  po...   \n",
       " 385    2018-07-03 12:22:32  PUBfr \"La STIBMIVB joue avec les noms de stati...   \n",
       " 393    2018-07-03 12:21:52  La  joue avec les noms de stations en honneur ...   \n",
       " 398    2018-07-03 12:21:32  les chinois on vite cassée  mdrr on les a nike...   \n",
       " 413    2018-07-03 12:19:42                                Tout à fait ça        \n",
       " 437    2018-07-03 12:17:51  : Incroyable  point d'exclamation Les Diables ...   \n",
       " 438    2018-07-03 12:17:49  : Les japonais super solides, belle défense et...   \n",
       " 443    2018-07-03 12:17:06  : _ Papa, pourquoi ma soeur s'appelle Rose  po...   \n",
       " 487    2018-07-03 12:13:56  : Incroyable  point d'exclamation Les Diables ...   \n",
       " ...                    ...                                                ...   \n",
       " 24490  2018-07-02 18:09:43  Die fucking Japanners plakken verdomme meer da...   \n",
       " 24494  2018-07-02 18:09:21  :  Belgique - Japon \\n\\nW/ &amp;  \\n  Lukaku &...   \n",
       " 24495  2018-07-02 18:09:16       :   c mtn ou jamais nonante minutes à fond     \n",
       " 24502  2018-07-02 18:08:54  : La catapulte infernale des frères Tachibana,...   \n",
       " 24504  2018-07-02 18:08:50  : La catapulte infernale des frères Tachibana,...   \n",
       " 24507  2018-07-02 18:08:41  : Subs  Mignolet, Casteels, Vermaelen, Fellain...   \n",
       " 24511  2018-07-02 18:08:30                                  Come on Belgium     \n",
       " 24514  2018-07-02 18:08:18  Yoshi passe à Mario, Mario passe à Luigi qui p...   \n",
       " 24516  2018-07-02 18:08:09    :  Hi Jean-Marie    uitroepteken \\n  \\n\\n\\n\\n     \n",
       " 24518  2018-07-02 18:08:06  :  Belgique - Japon \\n\\nW/ &amp;  \\n  Lukaku &...   \n",
       " 24524  2018-07-02 18:07:57              : Wordt dit onze Wales vraagteken       \n",
       " 24528  2018-07-02 18:07:48          we gingen kijken op een groot scherm        \n",
       " 24533  2018-07-02 18:07:34  :  Belgique - Japon \\n\\nW/ &amp;  \\n  Lukaku &...   \n",
       " 24537  2018-07-02 18:07:26                                          Lets go     \n",
       " 24539  2018-07-02 18:07:19                                         belgium\\n    \n",
       " 24544  2018-07-02 18:07:10  : Il a l'air intéressant cet épisode d'Olive e...   \n",
       " 24546  2018-07-02 18:07:07        : Go Belgium  uitroepteken uitroepteken       \n",
       " 24548  2018-07-02 18:07:06  La catapulte infernale des frères Tachibana, c...   \n",
       " 24554  2018-07-02 18:06:53                          ALLEZZZZZ ÉCRASEZ LES       \n",
       " 24560  2018-07-02 18:06:43  :  Belgique - Japon \\n\\nW/ &amp;  \\n  Lukaku &...   \n",
       " 24568  2018-07-02 18:06:23                                          Drongen     \n",
       " 24569  2018-07-02 18:06:23  Ik weet nu al wie de man van de Match wordt ti...   \n",
       " 24571  2018-07-02 18:06:12                          We zijn er klaar voor.      \n",
       " 24586  2018-07-02 18:05:16  Het leukste aan naar het WK kijken: met mijn t...   \n",
       " 24587  2018-07-02 18:05:15  : Subs  Mignolet, Casteels, Vermaelen, Fellain...   \n",
       " 24602  2018-07-02 18:04:54       Let's goooooooo \\nC'mon HAZAAAAAAAAARD \\n      \n",
       " 24610  2018-07-02 18:04:42  Un match et la bonne p'tite bière qui va avec ...   \n",
       " 24634  2018-07-02 18:04:12  Il a l'air intéressant cet épisode d'Olive et ...   \n",
       " 24636  2018-07-02 18:04:11  Go Go  uitroepteken Go Go  uitroepteken Wij su...   \n",
       " 24638  2018-07-02 18:04:06  Go Rode Duivels -  supportert mee uitroepteken...   \n",
       " \n",
       "       Language emotion_nl emotion_en  emotion_fr     emotion  \n",
       " 19          fr                                                \n",
       " 20          fr                                                \n",
       " 31          fr                                                \n",
       " 59          fr                                                \n",
       " 60          nl                                                \n",
       " 75          fr                                                \n",
       " 84          fr                                                \n",
       " 93          fr                                                \n",
       " 97          nl                                                \n",
       " 101         fr                                                \n",
       " 151         fr                                                \n",
       " 198         nl                                                \n",
       " 211         nl                                                \n",
       " 215         fr                                                \n",
       " 228         fr                                                \n",
       " 311         fr                                                \n",
       " 348         fr                                                \n",
       " 349         nl                                                \n",
       " 353         fr                                                \n",
       " 357         fr                                                \n",
       " 377         fr                                                \n",
       " 379         fr                                                \n",
       " 385         fr                                                \n",
       " 393         fr                                                \n",
       " 398         fr    knipoog       wink  Clin d'œil  Clin d'œil  \n",
       " 413         fr                                                \n",
       " 437         fr                                                \n",
       " 438         fr                                                \n",
       " 443         fr                                                \n",
       " 487         fr                                                \n",
       " ...        ...        ...        ...         ...         ...  \n",
       " 24490       nl                                                \n",
       " 24494       nl                                                \n",
       " 24495       fr                                                \n",
       " 24502       fr                                                \n",
       " 24504       fr                                                \n",
       " 24507       nl                                                \n",
       " 24511       nl                                                \n",
       " 24514       fr                                                \n",
       " 24516       nl                                                \n",
       " 24518       nl                                                \n",
       " 24524       nl                                                \n",
       " 24528       nl                                                \n",
       " 24533       nl                                                \n",
       " 24537       fr                                                \n",
       " 24539       nl                                                \n",
       " 24544       fr                                                \n",
       " 24546       nl                                                \n",
       " 24548       fr                                                \n",
       " 24554       fr                                                \n",
       " 24560       nl                                                \n",
       " 24568       nl                                                \n",
       " 24569       nl                                                \n",
       " 24571       nl                                                \n",
       " 24586       nl                                                \n",
       " 24587       nl                                                \n",
       " 24602       nl                                                \n",
       " 24610       fr                                                \n",
       " 24634       fr                                                \n",
       " 24636       nl                                                \n",
       " 24638       nl                                                \n",
       " \n",
       " [2586 rows x 7 columns]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>language</th>\n",
       "      <th>prediction</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>excitement</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hope</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>excitement</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>excitement</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>excitement</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>excitement</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anger</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cynicism</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24377</th>\n",
       "      <td>2018-07-02 18:16:33</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>:  Succes vanavond uitroepteken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24392</th>\n",
       "      <td>2018-07-02 18:15:21</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Balbezit voor de jappen hallo  wakker worden  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24393</th>\n",
       "      <td>2018-07-02 18:15:17</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>:  Hi Jean-Marie    uitroepteken \\n  \\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24399</th>\n",
       "      <td>2018-07-02 18:14:59</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Die Japannertjes zijn wel echt formaatje mini ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24407</th>\n",
       "      <td>2018-07-02 18:14:18</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>:  Hi Jean-Marie    uitroepteken \\n  \\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24408</th>\n",
       "      <td>2018-07-02 18:14:04</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>:  Hi Jean-Marie    uitroepteken \\n  \\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24411</th>\n",
       "      <td>2018-07-02 18:13:52</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>:  Succes vanavond uitroepteken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24419</th>\n",
       "      <td>2018-07-02 18:13:24</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>: Die fucking Japanners plakken verdomme meer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24427</th>\n",
       "      <td>2018-07-02 18:12:53</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>:  Succes vanavond uitroepteken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24433</th>\n",
       "      <td>2018-07-02 18:12:27</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Let's gooo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24490</th>\n",
       "      <td>2018-07-02 18:09:43</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Die fucking Japanners plakken verdomme meer da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24494</th>\n",
       "      <td>2018-07-02 18:09:21</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>:  Belgique - Japon \\n\\nW/ &amp;amp;  \\n  Lukaku &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24507</th>\n",
       "      <td>2018-07-02 18:08:41</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>: Subs  Mignolet, Casteels, Vermaelen, Fellain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24511</th>\n",
       "      <td>2018-07-02 18:08:30</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Come on Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24516</th>\n",
       "      <td>2018-07-02 18:08:09</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>:  Hi Jean-Marie    uitroepteken \\n  \\n\\n\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24518</th>\n",
       "      <td>2018-07-02 18:08:06</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>:  Belgique - Japon \\n\\nW/ &amp;amp;  \\n  Lukaku &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24524</th>\n",
       "      <td>2018-07-02 18:07:57</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>: Wordt dit onze Wales vraagteken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24528</th>\n",
       "      <td>2018-07-02 18:07:48</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we gingen kijken op een groot scherm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24533</th>\n",
       "      <td>2018-07-02 18:07:34</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>:  Belgique - Japon \\n\\nW/ &amp;amp;  \\n  Lukaku &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24539</th>\n",
       "      <td>2018-07-02 18:07:19</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>belgium\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24546</th>\n",
       "      <td>2018-07-02 18:07:07</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>: Go Belgium  uitroepteken uitroepteken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24560</th>\n",
       "      <td>2018-07-02 18:06:43</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>:  Belgique - Japon \\n\\nW/ &amp;amp;  \\n  Lukaku &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24568</th>\n",
       "      <td>2018-07-02 18:06:23</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Drongen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24569</th>\n",
       "      <td>2018-07-02 18:06:23</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ik weet nu al wie de man van de Match wordt ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24571</th>\n",
       "      <td>2018-07-02 18:06:12</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We zijn er klaar voor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24586</th>\n",
       "      <td>2018-07-02 18:05:16</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Het leukste aan naar het WK kijken: met mijn t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24587</th>\n",
       "      <td>2018-07-02 18:05:15</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>: Subs  Mignolet, Casteels, Vermaelen, Fellain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24602</th>\n",
       "      <td>2018-07-02 18:04:54</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Let's goooooooo \\nC'mon HAZAAAAAAAAARD \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24636</th>\n",
       "      <td>2018-07-02 18:04:11</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Go Go  uitroepteken Go Go  uitroepteken Wij su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24638</th>\n",
       "      <td>2018-07-02 18:04:06</td>\n",
       "      <td>nl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Go Rode Duivels -  supportert mee uitroepteken...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2433 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_at language  prediction  \\\n",
       "0                      NaN      NaN    cynicism   \n",
       "1                      NaN      NaN    cynicism   \n",
       "2                      NaN      NaN    cynicism   \n",
       "3                      NaN      NaN       anger   \n",
       "4                      NaN      NaN    cynicism   \n",
       "5                      NaN      NaN  excitement   \n",
       "6                      NaN      NaN       anger   \n",
       "7                      NaN      NaN        hope   \n",
       "8                      NaN      NaN       anger   \n",
       "9                      NaN      NaN    cynicism   \n",
       "10                     NaN      NaN    cynicism   \n",
       "11                     NaN      NaN     neutral   \n",
       "12                     NaN      NaN    cynicism   \n",
       "13                     NaN      NaN    cynicism   \n",
       "14                     NaN      NaN    cynicism   \n",
       "15                     NaN      NaN    cynicism   \n",
       "16                     NaN      NaN    cynicism   \n",
       "17                     NaN      NaN    cynicism   \n",
       "18                     NaN      NaN  excitement   \n",
       "19                     NaN      NaN  excitement   \n",
       "20                     NaN      NaN     neutral   \n",
       "21                     NaN      NaN  excitement   \n",
       "22                     NaN      NaN     neutral   \n",
       "23                     NaN      NaN    cynicism   \n",
       "24                     NaN      NaN    cynicism   \n",
       "25                     NaN      NaN  excitement   \n",
       "26                     NaN      NaN    cynicism   \n",
       "27                     NaN      NaN       anger   \n",
       "28                     NaN      NaN    cynicism   \n",
       "29                     NaN      NaN    cynicism   \n",
       "...                    ...      ...         ...   \n",
       "24377  2018-07-02 18:16:33       nl         NaN   \n",
       "24392  2018-07-02 18:15:21       nl         NaN   \n",
       "24393  2018-07-02 18:15:17       nl         NaN   \n",
       "24399  2018-07-02 18:14:59       nl         NaN   \n",
       "24407  2018-07-02 18:14:18       nl         NaN   \n",
       "24408  2018-07-02 18:14:04       nl         NaN   \n",
       "24411  2018-07-02 18:13:52       nl         NaN   \n",
       "24419  2018-07-02 18:13:24       nl         NaN   \n",
       "24427  2018-07-02 18:12:53       nl         NaN   \n",
       "24433  2018-07-02 18:12:27       nl         NaN   \n",
       "24490  2018-07-02 18:09:43       nl         NaN   \n",
       "24494  2018-07-02 18:09:21       nl         NaN   \n",
       "24507  2018-07-02 18:08:41       nl         NaN   \n",
       "24511  2018-07-02 18:08:30       nl         NaN   \n",
       "24516  2018-07-02 18:08:09       nl         NaN   \n",
       "24518  2018-07-02 18:08:06       nl         NaN   \n",
       "24524  2018-07-02 18:07:57       nl         NaN   \n",
       "24528  2018-07-02 18:07:48       nl         NaN   \n",
       "24533  2018-07-02 18:07:34       nl         NaN   \n",
       "24539  2018-07-02 18:07:19       nl         NaN   \n",
       "24546  2018-07-02 18:07:07       nl         NaN   \n",
       "24560  2018-07-02 18:06:43       nl         NaN   \n",
       "24568  2018-07-02 18:06:23       nl         NaN   \n",
       "24569  2018-07-02 18:06:23       nl         NaN   \n",
       "24571  2018-07-02 18:06:12       nl         NaN   \n",
       "24586  2018-07-02 18:05:16       nl         NaN   \n",
       "24587  2018-07-02 18:05:15       nl         NaN   \n",
       "24602  2018-07-02 18:04:54       nl         NaN   \n",
       "24636  2018-07-02 18:04:11       nl         NaN   \n",
       "24638  2018-07-02 18:04:06       nl         NaN   \n",
       "\n",
       "                                                    text  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2                                                    NaN  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "5                                                    NaN  \n",
       "6                                                    NaN  \n",
       "7                                                    NaN  \n",
       "8                                                    NaN  \n",
       "9                                                    NaN  \n",
       "10                                                   NaN  \n",
       "11                                                   NaN  \n",
       "12                                                   NaN  \n",
       "13                                                   NaN  \n",
       "14                                                   NaN  \n",
       "15                                                   NaN  \n",
       "16                                                   NaN  \n",
       "17                                                   NaN  \n",
       "18                                                   NaN  \n",
       "19                                                   NaN  \n",
       "20                                                   NaN  \n",
       "21                                                   NaN  \n",
       "22                                                   NaN  \n",
       "23                                                   NaN  \n",
       "24                                                   NaN  \n",
       "25                                                   NaN  \n",
       "26                                                   NaN  \n",
       "27                                                   NaN  \n",
       "28                                                   NaN  \n",
       "29                                                   NaN  \n",
       "...                                                  ...  \n",
       "24377             :  Succes vanavond uitroepteken         \n",
       "24392  Balbezit voor de jappen hallo  wakker worden  ...  \n",
       "24393    :  Hi Jean-Marie    uitroepteken \\n  \\n\\n\\n\\n    \n",
       "24399  Die Japannertjes zijn wel echt formaatje mini ...  \n",
       "24407    :  Hi Jean-Marie    uitroepteken \\n  \\n\\n\\n\\n    \n",
       "24408    :  Hi Jean-Marie    uitroepteken \\n  \\n\\n\\n\\n    \n",
       "24411             :  Succes vanavond uitroepteken         \n",
       "24419  : Die fucking Japanners plakken verdomme meer ...  \n",
       "24427             :  Succes vanavond uitroepteken         \n",
       "24433                                    Let's gooo       \n",
       "24490  Die fucking Japanners plakken verdomme meer da...  \n",
       "24494  :  Belgique - Japon \\n\\nW/ &amp;  \\n  Lukaku &...  \n",
       "24507  : Subs  Mignolet, Casteels, Vermaelen, Fellain...  \n",
       "24511                                  Come on Belgium    \n",
       "24516    :  Hi Jean-Marie    uitroepteken \\n  \\n\\n\\n\\n    \n",
       "24518  :  Belgique - Japon \\n\\nW/ &amp;  \\n  Lukaku &...  \n",
       "24524              : Wordt dit onze Wales vraagteken      \n",
       "24528          we gingen kijken op een groot scherm       \n",
       "24533  :  Belgique - Japon \\n\\nW/ &amp;  \\n  Lukaku &...  \n",
       "24539                                         belgium\\n   \n",
       "24546        : Go Belgium  uitroepteken uitroepteken      \n",
       "24560  :  Belgique - Japon \\n\\nW/ &amp;  \\n  Lukaku &...  \n",
       "24568                                          Drongen    \n",
       "24569  Ik weet nu al wie de man van de Match wordt ti...  \n",
       "24571                          We zijn er klaar voor.     \n",
       "24586  Het leukste aan naar het WK kijken: met mijn t...  \n",
       "24587  : Subs  Mignolet, Casteels, Vermaelen, Fellain...  \n",
       "24602       Let's goooooooo \\nC'mon HAZAAAAAAAAARD \\n     \n",
       "24636  Go Go  uitroepteken Go Go  uitroepteken Wij su...  \n",
       "24638  Go Rode Duivels -  supportert mee uitroepteken...  \n",
       "\n",
       "[2433 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FinalDataNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
